<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maths on </title>
    <link>https://www.danielsobrado.com/categories/maths/</link>
    <description>Recent content in Maths on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 03 Jan 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.danielsobrado.com/categories/maths/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Logistic Regression</title>
      <link>https://www.danielsobrado.com/post/logistic-regression/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/logistic-regression/</guid>
      <description>Introduction to Linear Regression  A linear regression tries to estimate a linear relationship that best fits a given set of data.
 We need to understand that Linear Regression won´t help us with non linear relationships.
When we do a regression we are trying to understand the strength and direction of the relationship between two or more variables.
This is different from correlation analysis, because the model allows us to infer on new inputs.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://www.danielsobrado.com/post/linear-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/linear-regression/</guid>
      <description>Introduction to Linear Regression  A linear regression tries to estimate a linear relationship that best fits a given set of data.
 We need to understand that Linear Regression won´t help us with non linear relationships.
When we do a regression we are trying to understand the strength and direction of the relationship between two or more variables.
This is different from correlation analysis, because the model allows us to infer on new inputs.</description>
    </item>
    
    <item>
      <title>Variational Autoencoders</title>
      <link>https://www.danielsobrado.com/post/variational-autoencoders/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/variational-autoencoders/</guid>
      <description>Autoencoders An autoencoder takes data with a large amount of parameters and tries to compress it into a smaller representation.
 Encoder:
 Bottleneck
 Decoder:
  Variational Autoencoders Variational Autoencoders is a technique to compress data</description>
    </item>
    
    <item>
      <title>Fast Fourier Transform</title>
      <link>https://www.danielsobrado.com/post/fast-fourier-transform/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/fast-fourier-transform/</guid>
      <description>Move static content to static asdsdswd Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like
▾ &amp;lt;root&amp;gt;/ ▾ images/ logo.png  should become
▾ &amp;lt;root&amp;gt;/ ▾ static/ ▾ images/ logo.png  Additionally, you&amp;rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.</description>
    </item>
    
    <item>
      <title>Entropy, Cross-Entropy and Kullback-Leibler Divergence</title>
      <link>https://www.danielsobrado.com/post/entropy-cross-entropy-and-kullback-leibler-divergence/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/entropy-cross-entropy-and-kullback-leibler-divergence/</guid>
      <description>Information theory A bit is a number that is equal to 0 or 1. In Information theory, receiving a bit reduces our uncertainty by half, or a factor of 2.
If our clients in the bank are likely to repay a loan or not, and we have our risk manager to tell us if a certain client repaid or not his loan, if the chance of the client being good or bad is 50%, our risk manager is reducing our uncertainty about the client by a factor of 2.</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://www.danielsobrado.com/post/naive-bayes/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/naive-bayes/</guid>
      <description>Introduction In Bayesian statistics there are two important concepts, we use probabilities to measure the uncertainty about the parameters used by the probability distributions, and we use the Bayes´ theorem to update those probabilities.
Naive Bayes Types of Naive Bayes Algorithms Gaussian Naive Bayes We assume that each class has continuous Normal/Gaussian distributed values.
$$ P \left( x _ { i } | y \right) = \frac { 1 } { \sqrt { 2 \pi \sigma _ { y } ^ { 2 } } } \exp \left( - \frac { \left( x _ { i } - \mu _ { y } \right) ^ { 2 } } { 2 \sigma _ { y } ^ { 2 } } \right) $$</description>
    </item>
    
    <item>
      <title>Prior and Posterior distributions</title>
      <link>https://www.danielsobrado.com/post/prior-and-posterior-distributions/</link>
      <pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/prior-and-posterior-distributions/</guid>
      <description>Let´s suppose that we know all the possible causes for an outcome, for example:
**
Bayes theorem Posterior = ( Likelihood * Prior ) / Evidence
Here, P(movie|Sci-fi) is called Posterior, P(Sci-fi|Movie) is Likelihood, P(movie) is Prior, P(Sci-fi) is Evidence.
Prior: How probable was our hypothesis before observing the evidence? Posterior: How probable is our hypothesis given the observed evidence? Evidence: How probable is the new evidence under all possible hypotheses?</description>
    </item>
    
    <item>
      <title>Basic probability concepts</title>
      <link>https://www.danielsobrado.com/post/basic-probability-concepts/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/basic-probability-concepts/</guid>
      <description>Introduction In this series of articles, We&amp;rsquo;ll dig deep into understanding Bayesian inference, starting from the basics.
The main idea behind Bayesian statistics is the Bayes theorem, we need to understand some concepts first.
TL;DR Let´s take the following poker cards as an example:
Our experiment is to take two cards for the deck, one at a time, each card extraction is an event. We define as outcomes that a card is of a defined</description>
    </item>
    
    <item>
      <title>Probability distributions</title>
      <link>https://www.danielsobrado.com/post/probability-distributions/</link>
      <pubDate>Sat, 08 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/probability-distributions/</guid>
      <description>Introduction Distributions a laws governing these are a must know for every data scientist.
The law of large numbers The law of large numbers states that the more samples we collect the more close the sample mean will be to the population mean.
Central Limit Theorem  The sampling distribution of the mean of any independent, random variable will be normal or nearly normal, if the sample size is large enough.</description>
    </item>
    
    <item>
      <title>Numpy: Doing some maths</title>
      <link>https://www.danielsobrado.com/post/numpy-doing-some-maths/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/numpy-doing-some-maths/</guid>
      <description> Linear Algebra Matrix object This object is always two dimensional, and it doesn´t use the default broadcasting from ndarray.
We can create an identity matrix using np.eye:
identity = np.eye(3) identity Output: $ array([[ 1., 0., 0.], $ [ 0., 1., 0.], $ [ 0., 0., 1.]])  array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]])  Statistics Reading and writing </description>
    </item>
    
  </channel>
</rss>