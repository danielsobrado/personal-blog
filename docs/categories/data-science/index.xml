<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on </title>
    <link>https://www.danielsobrado.com/categories/data-science/</link>
    <description>Recent content in Data Science on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 25 Sep 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.danielsobrado.com/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow vs Pytorch: Basics</title>
      <link>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-basics/</link>
      <pubDate>Sun, 25 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-basics/</guid>
      <description>Introduction The best way to learn a framework is to learn two at the same time and compare how things are being achieved in different ways, understanding the advantages and disadvantages.
Tensorflow and Pytorch are frameworks for fast tensor manipulation that is what is required for deep learning and some other machine learning methods.
Both heavily oriented towards machine learning and especially deep learning are low-level libraries to operate on tensors (n-dimensional arrays).</description>
    </item>
    
    <item>
      <title>Likelihood encoding</title>
      <link>https://www.danielsobrado.com/post/likelihood-encoding/</link>
      <pubDate>Mon, 12 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/likelihood-encoding/</guid>
      <description>Introduction Also known as mean encoding, impact encoding or target encoding, it is a technique
Mean Absolute Error (MAE) Measures average/mean squared error of our predictions.
$$ MAE = \frac{1}{n} \sum |yi - \hat{y}_i| $$ Gives less weight to the outliers, when you are sure that they are outliers prefer MAE to MSE.
Mean Absolute Percentage Error (MAPE) &amp;hellip;
$$ MAPE = \frac{100}{n} \sumi^n \frac{yi - \hat{y}i}{yi} $$ MAPE is</description>
    </item>
    
    <item>
      <title>Loss functions: MAE, MAPE, MSE, RMSE and RMSLE</title>
      <link>https://www.danielsobrado.com/post/loss-functions-mae-mape-mse-rmse-and-rmsle/</link>
      <pubDate>Sat, 23 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/loss-functions-mae-mape-mse-rmse-and-rmsle/</guid>
      <description>Loss Functions The loss function calculates the difference between the output of your model and the &amp;ldquo;Ground Truth&amp;rdquo; or actual values.
All this functiones measure the ratio between actual/reference and predicted, the differences are in how the outliers impact the final outcome.
Each metric has its own strenghts and weakness and its fit for a different use case, we need to understand how these metrics impact our results, and how to interpret them, if they give us a relative or absulute value, the unit being used by the metric and how to use multiple metrics to understand where the loss/error is coming from.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://www.danielsobrado.com/post/logistic-regression/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/logistic-regression/</guid>
      <description>Introduction to Linear Regression  A linear regression tries to estimate a linear relationship that best fits a given set of data.
 We need to understand that Linear Regression won´t help us with non linear relationships.
When we do a regression we are trying to understand the strength and direction of the relationship between two or more variables.
This is different from correlation analysis, because the model allows us to infer on new inputs.</description>
    </item>
    
    <item>
      <title>Analysis of residuals</title>
      <link>https://www.danielsobrado.com/post/analysis-of-residuals/</link>
      <pubDate>Sat, 11 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/analysis-of-residuals/</guid>
      <description> Introduction conda install statsmodels seaborn
Plotting residuals Regression assumptions Linearity and equal variance Normality
Interpreting residuals Common issues Heteroscedasticity Identify heteroscedasticity Breusch-Pagan Lagrange Multiplier test Using StatsModels we have statsmodels.stats.diagnostic.het_breuschpagan
Non-linearity Outliers Long Y-axis datapoints X-axis unbalanced </description>
    </item>
    
    <item>
      <title>Tensorflow vs Pytorch: Linear Regression</title>
      <link>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-linear-regression/</link>
      <pubDate>Tue, 25 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-linear-regression/</guid>
      <description>Introduction to Linear Regression The best way to learn a framework is to learn two at the same time and compare how things are being achieved in different ways, understanding the advantages and disadvantages.
Tensorflow and Pytorch are frameworks for fast tensor manipulation that is what is required for deep learning and some other machine learning methods.
Both heavily oriented towards machine learning and especially deep learning are low-level libraries to operate on tensors (n-dimensional arrays).</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://www.danielsobrado.com/post/linear-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/linear-regression/</guid>
      <description>Introduction to Linear Regression  A linear regression tries to estimate a linear relationship that best fits a given set of data.
 We need to understand that Linear Regression won´t help us with non linear relationships.
When we do a regression we are trying to understand the strength and direction of the relationship between two or more variables.
This is different from correlation analysis, because the model allows us to infer on new inputs.</description>
    </item>
    
    <item>
      <title>Variational Autoencoders</title>
      <link>https://www.danielsobrado.com/post/variational-autoencoders/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/variational-autoencoders/</guid>
      <description>Autoencoders An autoencoder takes data with a large amount of parameters and tries to compress it into a smaller representation.
 Encoder:
 Bottleneck
 Decoder:
  Variational Autoencoders Variational Autoencoders is a technique to compress data</description>
    </item>
    
    <item>
      <title>RFM Scoring</title>
      <link>https://www.danielsobrado.com/post/rfm-scoring/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/rfm-scoring/</guid>
      <description>Why it matters? Customer segmentation nowadays is a basic tool of any bank, you want to classify your customers naturally and understand how cluster shift and evolve in time.
It customer cluster needs to be treated in a different way from multiple angles:
 Channels: Is a tech savvy customer? Or it is easier to call him? Is he suing tech but still going to the branch and we should advise him on more efficient ways to do his transactions?</description>
    </item>
    
    <item>
      <title>Customer Segmentation</title>
      <link>https://www.danielsobrado.com/post/customer-segmentation/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/customer-segmentation/</guid>
      <description>Why it matters? Customer segmentation nowadays is a basic tool of any bank, you want to classify your customers naturally and understand how cluster shift and evolve in time.
It customer cluster needs to be treated in a different way from multiple angles:
 Channels: Is a tech savvy customer? Or it is easier to call him? Is he suing tech but still going to the branch and we should advise him on more efficient ways to do his transactions?</description>
    </item>
    
    <item>
      <title>Customer Segmentation</title>
      <link>https://www.danielsobrado.com/post/customer-segmentation/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/customer-segmentation/</guid>
      <description>Why it matters? Customer segmentation nowadays is a basic tool of any bank, you want to classify your customers naturally and understand how cluster shift and evolve in time.
It customer cluster needs to be treated in a different way from multiple angles:
 Channels: Is a tech savvy customer? Or it is easier to call him? Is he suing tech but still going to the branch and we should advise him on more efficient ways to do his transactions?</description>
    </item>
    
    <item>
      <title>Customer Segmentation</title>
      <link>https://www.danielsobrado.com/post/customer-segmentation/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/customer-segmentation/</guid>
      <description>Why it matters? Customer segmentation nowadays is a basic tool of any bank, you want to classify your customers naturally and understand how cluster shift and evolve in time.
It customer cluster needs to be treated in a different way from multiple angles:
 Channels: Is a tech savvy customer? Or it is easier to call him? Is he suing tech but still going to the branch and we should advise him on more efficient ways to do his transactions?</description>
    </item>
    
    <item>
      <title>Scipy: Introduction</title>
      <link>https://www.danielsobrado.com/post/scipy-introduction/</link>
      <pubDate>Mon, 28 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/scipy-introduction/</guid>
      <description> Introduction </description>
    </item>
    
    <item>
      <title>Numpy: ndarrays</title>
      <link>https://www.danielsobrado.com/post/numpy-ndarrays/</link>
      <pubDate>Mon, 29 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/numpy-ndarrays/</guid>
      <description>Numpy lets us create arrays in multiple ways, most of the time in consonancy with core Python and other libraries like Pandas.
Creating ndarrays We can get Numpy vector and a matrix rapidly from a Python list:
vector = np.array([1,2,3,4]) vector Output: $ array([1, 2, 3, 4])  matrix = vector.reshape((2,2)) matrix Output: $ array([[1, 2], [3, 4]])   We can quickly create matrices of ones and zeros:</description>
    </item>
    
    <item>
      <title>Numpy: Introduction</title>
      <link>https://www.danielsobrado.com/post/numpy-introduction/</link>
      <pubDate>Sun, 28 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/numpy-introduction/</guid>
      <description>Introduction Numpy is the core package for scientific computing, it has capabilities for fast processing of n-dimensional arrays and in general linear algebra.
There are multiple other well known packages in data science that rely on Numpy like Pandas and Scipy.
Installing Numpy For the examples we´ll just use pip to install Numpy, ideally it will be inside a container like Anaconda:
$ pip install numpy  Numpy Arrays ndarray is the earth of NumPy, it&amp;rsquo;s the main data storage object of the framework.</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://www.danielsobrado.com/post/naive-bayes/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/naive-bayes/</guid>
      <description>Introduction In Bayesian statistics there are two important concepts, we use probabilities to measure the uncertainty about the parameters used by the probability distributions, and we use the Bayes´ theorem to update those probabilities.
Naive Bayes Types of Naive Bayes Algorithms Gaussian Naive Bayes We assume that each class has continuous Normal/Gaussian distributed values.
$$ P \left( x _ { i } | y \right) = \frac { 1 } { \sqrt { 2 \pi \sigma _ { y } ^ { 2 } } } \exp \left( - \frac { \left( x _ { i } - \mu _ { y } \right) ^ { 2 } } { 2 \sigma _ { y } ^ { 2 } } \right) $$</description>
    </item>
    
    <item>
      <title>Worldbank datasets: Jobs and economic indicators</title>
      <link>https://www.danielsobrado.com/post/worldbank-datasets-jobs-and-economic-indicators/</link>
      <pubDate>Fri, 28 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/worldbank-datasets-jobs-and-economic-indicators/</guid>
      <description> Introduction </description>
    </item>
    
    <item>
      <title>Numpy: Doing some maths</title>
      <link>https://www.danielsobrado.com/post/numpy-doing-some-maths/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/numpy-doing-some-maths/</guid>
      <description> Linear Algebra Matrix object This object is always two dimensional, and it doesn´t use the default broadcasting from ndarray.
We can create an identity matrix using np.eye:
identity = np.eye(3) identity Output: $ array([[ 1., 0., 0.], $ [ 0., 1., 0.], $ [ 0., 0., 1.]])  array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]])  Statistics Reading and writing </description>
    </item>
    
  </channel>
</rss>