<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on </title>
    <link>https://www.danielsobrado.com/tags/spark/</link>
    <description>Recent content in Spark on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 10 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.danielsobrado.com/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Apache Spark: Properties precedence</title>
      <link>https://www.danielsobrado.com/post/apache-spark-properties-precedence/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/apache-spark-properties-precedence/</guid>
      <description>Proper Properties When starting with Spark jobs, one of the most common tasks is to understand how to finetune properties.
It is critical to define the right properties for your job, to avoid it to fail, or to take too long, at the same time you don&amp;rsquo;t want to be too greedy with the resources of your cluster, some might complain!
The problem When your codebase grows and you need some tools and you write some decent amount of code, you cannot just rely on an editor to edit the code and launch the job from the command line.</description>
    </item>
    
    <item>
      <title>Apache Spark: Introduction to project Tungsten</title>
      <link>https://www.danielsobrado.com/post/apache-spark-introduction-to-project-tungsten/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/apache-spark-introduction-to-project-tungsten/</guid>
      <description>Introduction Project Tungsten is available from Spark 1.4, Spark 2.x comes with the second generation of the Tungsten engine.
Tungsten is a compiler that applies to queries and generates optimized bytecode at runtime.
 Tungsten compiles your queries/stages into single bytecode JVM function that improve CPU efficiency and gain performance.
 This is one of those things that you could live without knowing about it and still do fine in Spark programming, but is extremely interesting and can be useful for advanced optimizations and to understand the insides of Spark.</description>
    </item>
    
  </channel>
</rss>