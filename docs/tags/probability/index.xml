<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability on </title>
    <link>https://www.danielsobrado.com/tags/probability/</link>
    <description>Recent content in Probability on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 07 Apr 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.danielsobrado.com/tags/probability/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Naive Bayes</title>
      <link>https://www.danielsobrado.com/post/naive-bayes/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/naive-bayes/</guid>
      <description>Introduction In Bayesian statistics there are two important concepts, we use probabilities to measure the uncertainty about the parameters used by the probability distributions, and we use the Bayes´ theorem to update those probabilities.
Naive Bayes Types of Naive Bayes Algorithms Gaussian Naive Bayes We assume that each class has continuous Normal/Gaussian distributed values.
$$ P \left( x _ { i } | y \right) = \frac { 1 } { \sqrt { 2 \pi \sigma _ { y } ^ { 2 } } } \exp \left( - \frac { \left( x _ { i } - \mu _ { y } \right) ^ { 2 } } { 2 \sigma _ { y } ^ { 2 } } \right) $$ sklearn.</description>
    </item>
    
    <item>
      <title>Prior and Posterior distributions</title>
      <link>https://www.danielsobrado.com/post/prior-and-posterior-distributions/</link>
      <pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/prior-and-posterior-distributions/</guid>
      <description>Let´s suppose that we know all the possible causes for an outcome, for example:
**
Bayes theorem Posterior = ( Likelihood * Prior ) / Evidence
Here, P(movie|Sci-fi) is called Posterior, P(Sci-fi|Movie) is Likelihood, P(movie) is Prior, P(Sci-fi) is Evidence.
Prior: How probable was our hypothesis before observing the evidence? Posterior: How probable is our hypothesis given the observed evidence? Evidence: How probable is the new evidence under all possible hypotheses?</description>
    </item>
    
    <item>
      <title>Basic probability concepts</title>
      <link>https://www.danielsobrado.com/post/basic-probability-concepts/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/basic-probability-concepts/</guid>
      <description>Introduction In this series of articles, We&amp;rsquo;ll dig deep into understanding Bayesian inference, starting from the basics.
The main idea behind Bayesian statistics is the Bayes theorem, we need to understand some concepts first.
TL;DR Let´s take the following poker cards as an example:
Our experiment is to take two cards for the deck, one at a time, each card extraction is an event. We define as outcomes that a card is of a defined</description>
    </item>
    
    <item>
      <title>Probability distributions</title>
      <link>https://www.danielsobrado.com/post/probability-distributions/</link>
      <pubDate>Sat, 08 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/probability-distributions/</guid>
      <description>Introduction Distributions a laws governing these are a must know for every data scientist.
The law of large numbers The law of large numbers states that the more samples we collect the more close the sample mean will be to the population mean.
Central Limit Theorem  The sampling distribution of the mean of any independent, random variable will be normal or nearly normal, if the sample size is large enough.</description>
    </item>
    
  </channel>
</rss>