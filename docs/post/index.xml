<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://www.danielsobrado.com/post/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Fri, 28 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.danielsobrado.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Apache Spark: Introduction to project Tungsten</title>
      <link>https://www.danielsobrado.com/post/apache-spark-introduction-to-project-tungsten/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/apache-spark-introduction-to-project-tungsten/</guid>
      <description>Introduction Project Tungsten is available from Spark 1.4, Spark 2.x comes with the second generation of the Tungsten engine.
Tungsten is a compiler that applies to queries and generates optimized bytecode at runtime.
 Tungsten compiles your queries/stages into single bytecode JVM function that improve CPU efficiency and gain performance.
 This is one of those things that you could live without knowing about it and still do fine in Spark programming, but is extremely interesting and can be useful for advanced optimizations and to understand the insides of Spark.</description>
    </item>
    
    <item>
      <title>Tensorflow vs Pytorch: Basics</title>
      <link>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-basics/</link>
      <pubDate>Sun, 25 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-basics/</guid>
      <description>Introduction The best way to learn a framework is to learn two at the same time and compare how things are being achieved in different ways, understanding the advantages and disadvantages.
Tensorflow and Pytorch are frameworks for fast tensor manipulation that is what is required for deep learning and some other machine learning methods.
Both heavily oriented towards machine learning and especially deep learning are low-level libraries to operate on tensors (n-dimensional arrays).</description>
    </item>
    
    <item>
      <title>Likelihood encoding</title>
      <link>https://www.danielsobrado.com/post/likelihood-encoding/</link>
      <pubDate>Mon, 12 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/likelihood-encoding/</guid>
      <description>Introduction Also known as mean encoding, impact encoding or target encoding, it is a technique
Mean Absolute Error (MAE) Measures average/mean squared error of our predictions.
$$ MAE = \frac{1}{n} \sum |yi - \hat{y}_i| $$ Gives less weight to the outliers, when you are sure that they are outliers prefer MAE to MSE.
Mean Absolute Percentage Error (MAPE) &amp;hellip;
$$ MAPE = \frac{100}{n} \sumi^n \frac{yi - \hat{y}i}{yi} $$ MAPE is</description>
    </item>
    
    <item>
      <title>Loss functions: MAE, MAPE, MSE, RMSE and RMSLE</title>
      <link>https://www.danielsobrado.com/post/loss-functions-mae-mape-mse-rmse-and-rmsle/</link>
      <pubDate>Sat, 23 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/loss-functions-mae-mape-mse-rmse-and-rmsle/</guid>
      <description>Loss Functions The loss function calculates the difference between the output of your model and the &amp;ldquo;Ground Truth&amp;rdquo; or actual values.
All this functiones measure the ratio between actual/reference and predicted, the differences are in how the outliers impact the final outcome.
Each metric has its own strenghts and weakness and its fit for a different use case, we need to understand how these metrics impact our results, and how to interpret them, if they give us a relative or absulute value, the unit being used by the metric and how to use multiple metrics to understand where the loss/error is coming from.</description>
    </item>
    
    <item>
      <title>Analysis of residuals</title>
      <link>https://www.danielsobrado.com/post/analysis-of-residuals/</link>
      <pubDate>Sat, 11 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/analysis-of-residuals/</guid>
      <description> Introduction conda install statsmodels seaborn
Plotting residuals Regression assumptions Linearity and equal variance Normality
Interpreting residuals Common issues Heteroscedasticity Identify heteroscedasticity Breusch-Pagan Lagrange Multiplier test Using StatsModels we have statsmodels.stats.diagnostic.het_breuschpagan
Non-linearity Outliers Long Y-axis datapoints X-axis unbalanced </description>
    </item>
    
    <item>
      <title>Tensorflow vs Pytorch: Linear Regression</title>
      <link>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-linear-regression/</link>
      <pubDate>Tue, 25 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/tensorflow-vs-pytorch-linear-regression/</guid>
      <description>Introduction to Linear Regression The best way to learn a framework is to learn two at the same time and compare how things are being achieved in different ways, understanding the advantages and disadvantages.
Tensorflow and Pytorch are frameworks for fast tensor manipulation that is what is required for deep learning and some other machine learning methods.
Both heavily oriented towards machine learning and especially deep learning are low-level libraries to operate on tensors (n-dimensional arrays).</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://www.danielsobrado.com/post/linear-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/linear-regression/</guid>
      <description>Introduction to Linear Regression  A linear regression tries to estimate a linear relationship that best fits a given set of data.
 We need to understand that Linear Regression won´t help us with non linear relationships.
When we do a regression we are trying to understand the strength and direction of the relationship between two or more variables.
This is different from correlation analysis, because the model allows us to infer on new inputs.</description>
    </item>
    
    <item>
      <title>Kaggle: Mercari competition</title>
      <link>https://www.danielsobrado.com/post/kaggle-mercari-competition/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/kaggle-mercari-competition/</guid>
      <description>Step 1. Install Hugo Goto hugo releases and download the appropriate version for your os and architecture.
Save it somewhere specific as we will be using it in the next step.
More complete instructions are available at installing hugo
Step 2. Build the Docs Hugo has its own example site which happens to also be the documentation site you are reading right now.
Follow the following steps:
 Clone the hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313  Corresponding pseudo commands:</description>
    </item>
    
    <item>
      <title>Fast Fourier Transform</title>
      <link>https://www.danielsobrado.com/post/fast-fourier-transform/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/fast-fourier-transform/</guid>
      <description>Move static content to static asdsdswd Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like
▾ &amp;lt;root&amp;gt;/ ▾ images/ logo.png  should become
▾ &amp;lt;root&amp;gt;/ ▾ static/ ▾ images/ logo.png  Additionally, you&amp;rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.</description>
    </item>
    
    <item>
      <title>Scipy: Introduction</title>
      <link>https://www.danielsobrado.com/post/scipy-introduction/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/scipy-introduction/</guid>
      <description> Introduction </description>
    </item>
    
    <item>
      <title>Let&#39;s discuss Microservices</title>
      <link>https://www.danielsobrado.com/post/lets-discuss-microservices/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/lets-discuss-microservices/</guid>
      <description>Introduction Microservices are based on the principle &amp;ldquo;divide and conquer&amp;rdquo;.
There is a trade-off between having too many services and too few or only one, and this depends on the scope of your project.
Add extra services provide some advantages but there are also disadvantages, having too many services is extreme, having only one service (Monolith pattern) is also extreme (Nano-services antipattern), the right balance is where we want to be.</description>
    </item>
    
    <item>
      <title>CRUD vs CQRS</title>
      <link>https://www.danielsobrado.com/post/crud-vs-cqrs/</link>
      <pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/crud-vs-cqrs/</guid>
      <description> Introduction </description>
    </item>
    
    <item>
      <title>Numpy: ndarrays</title>
      <link>https://www.danielsobrado.com/post/numpy-ndarrays/</link>
      <pubDate>Mon, 29 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/numpy-ndarrays/</guid>
      <description>We have
Creating ndarrays We can get Numpy vector and a matrix rapidly from a Python list:
vector = np.array([1,2,3,4]) vector Output: $ array([1, 2, 3, 4])  matrix = vector.reshape((2,2)) matrix Output: $ array([[1, 2], [3, 4]])   We can quickly create matrices of ones and zeros:
all_zeros = np.zeros((3,3)) all_zeros Output: $ array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])  all_ones = np.ones((3,3)) all_ones Output: $ array([[1.</description>
    </item>
    
    <item>
      <title>Numpy: Introduction</title>
      <link>https://www.danielsobrado.com/post/numpy-introduction/</link>
      <pubDate>Sun, 28 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/numpy-introduction/</guid>
      <description>Introduction Numpy is the core package for scientific computing, it has capabilities for fast processing of n-dimensional arrays and in general linear algebra.
There are multiple other well known packages in data science that rely on Numpy like Pandas and Scipy.
Installing Numpy For the examples we´ll just use pip to install Numpy, ideally it will be inside a container like Anaconda:
$ pip install numpy  Numpy Arrays ndarray is the earth of NumPy, it&amp;rsquo;s the main data storage object of the framework.</description>
    </item>
    
    <item>
      <title>AngularJS: Introduction</title>
      <link>https://www.danielsobrado.com/post/angularjs-introduction/</link>
      <pubDate>Sat, 28 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/angularjs-introduction/</guid>
      <description>Introduction All started when I started developing some quick reconciliation tool for the bank´s Product Control team, someone said: &amp;ldquo;Hey Daniel, can you help us with this? Take this and that and apply this rules and give us the results on something we can visualize, it is too much data for excel!
I love solving issues for my business partners and I immediately started rolling my sleeves, things look simple and easy at the beginning but the reality sometimes is different, I had a to use a large database to transform the data, this triggered a proper project and many governance procedures, like authentication, authorization, Chinese walls, user management etc.</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://www.danielsobrado.com/post/naive-bayes/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/naive-bayes/</guid>
      <description>Introduction In Bayesian statistics there are two important concepts, we use probabilities to measure the uncertainty about the parameters used by the probability distributions, and we use the Bayes´ theorem to update those probabilities.
Naive Bayes Types of Naive Bayes Algorithms Gaussian Naive Bayes We assume that each class has continuous Normal/Gaussian distributed values.
$$ P \left( x _ { i } | y \right) = \frac { 1 } { \sqrt { 2 \pi \sigma _ { y } ^ { 2 } } } \exp \left( - \frac { \left( x _ { i } - \mu _ { y } \right) ^ { 2 } } { 2 \sigma _ { y } ^ { 2 } } \right) $$</description>
    </item>
    
    <item>
      <title>Prior and Posterior distributions</title>
      <link>https://www.danielsobrado.com/post/prior-and-posterior-distributions/</link>
      <pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/prior-and-posterior-distributions/</guid>
      <description>Let´s suppose that we know all the possible causes for an outcome, for example:
**
Bayes theorem Posterior = ( Likelihood * Prior ) / Evidence
Here, P(movie|Sci-fi) is called Posterior, P(Sci-fi|Movie) is Likelihood, P(movie) is Prior, P(Sci-fi) is Evidence.
Prior: How probable was our hypothesis before observing the evidence? Posterior: How probable is our hypothesis given the observed evidence? Evidence: How probable is the new evidence under all possible hypotheses?</description>
    </item>
    
    <item>
      <title>Basic probability concepts</title>
      <link>https://www.danielsobrado.com/post/basic-probability-concepts/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/basic-probability-concepts/</guid>
      <description>Introduction In this series of articles, We&amp;rsquo;ll dig deep into understanding Bayesian inference, starting from the basics.
The main idea behind Bayesian statistics is the Bayes theorem, we need to understand some concepts first.
TL;DR Let´s take the following poker cards as an example:
Our experiment is to take two cards for the deck, one at a time, each card extraction is an event. We define as outcomes that a card is of a defined</description>
    </item>
    
    <item>
      <title>Worldbank datasets: Jobs and economic indicators</title>
      <link>https://www.danielsobrado.com/post/worldbank-datasets-jobs-and-economic-indicators/</link>
      <pubDate>Fri, 28 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/worldbank-datasets-jobs-and-economic-indicators/</guid>
      <description> Introduction </description>
    </item>
    
    <item>
      <title>Probability distributions</title>
      <link>https://www.danielsobrado.com/post/probability-distributions/</link>
      <pubDate>Sat, 08 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/probability-distributions/</guid>
      <description>Introduction Distributions a laws governing these are a must know for every data scientist.
The law of large numbers The law of large numbers states that the more samples we collect the more close the sample mean will be to the population mean.
Central Limit Theorem  The sampling distribution of the mean of any independent, random variable will be normal or nearly normal, if the sample size is large enough.</description>
    </item>
    
    <item>
      <title>Functional Programming Basic Concepts</title>
      <link>https://www.danielsobrado.com/post/functional-programming-basic-concepts/</link>
      <pubDate>Wed, 08 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/functional-programming-basic-concepts/</guid>
      <description> 1. Introduction Functional programming is a programming paradigm, based on some principles
2. Advantages 3. Concepts Let´s enumerate and describe the main pillars of functional programming, each one deserves several separated articles:
3.1 Pure functions 3.2 Function composition 3.3. Avoid shared state 3.4 Avoid mutating state / Immutability 3.5 Avoid side effects 3.5.1 Monads </description>
    </item>
    
    <item>
      <title>Introduction to Data Science: Concepts</title>
      <link>https://www.danielsobrado.com/post/introduction-to-data-science-concepts/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/introduction-to-data-science-concepts/</guid>
      <description>So many new concepts for the beginner data scientist, let´s start one by one:
Tensor A tensor is a generalized matrix, it is defined by his rank, a rank zero tensor, is just a number, while a rank two tensor is a two dimensional matrix or array.
See: What’s the difference between a matrix and a tensor?
Features Data Splits  Training set: This is the biggest set where we run our learning algorithm on.</description>
    </item>
    
    <item>
      <title>The machine data ecosystem</title>
      <link>https://www.danielsobrado.com/post/the-machine-data-ecosystem/</link>
      <pubDate>Sat, 28 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/the-machine-data-ecosystem/</guid>
      <description>Machine Data Platforms There are two known platforms for machine data, Splunk and Elastic Stack, let&amp;rsquo;s do some research&amp;hellip;
The cost of the platforms Splunk is great but the price tag is not that great, Elastic Stack is Open Source but some of the Enterprise features and Advanced Analytics capabilities are licensed on X-Pack under a paywall, can we get around some of these capabilities, like security using other Open Source solutions?</description>
    </item>
    
    <item>
      <title>Introduction to Machine Data</title>
      <link>https://www.danielsobrado.com/post/introduction-to-machine-data/</link>
      <pubDate>Sat, 18 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/introduction-to-machine-data/</guid>
      <description>What is Machine Data With the rise of cheap storage and processing, we are in a better position to extract and process any type of data that can help with insights.
Any type of data produced by programs and processes is useful for us, we can use logs, network packets, any type of metrics and performance clues.
This means a large amount of unstructured data that is ready for our consumption and use.</description>
    </item>
    
    <item>
      <title>Functional programming in Java: Lambdas</title>
      <link>https://www.danielsobrado.com/post/functional-programming-in-java-lambdas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/functional-programming-in-java-lambdas/</guid>
      <description>Introduction A lambda expression in
Concepts Functional interface: Is an interface that declares one abstract method.
Target type: The type expected by a lambda expression.</description>
    </item>
    
    <item>
      <title>Numpy: Doing some maths</title>
      <link>https://www.danielsobrado.com/post/numpy-doing-some-maths/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielsobrado.com/post/numpy-doing-some-maths/</guid>
      <description> Linear Algebra Matrix object This object is always two dimensional, and it doesn´t use the default broadcasting from ndarray.
We can create an identity matrix using np.eye:
identity = np.eye(3) identity Output: $ array([[ 1., 0., 0.], $ [ 0., 1., 0.], $ [ 0., 0., 1.]])  array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]])  Statistics Reading and writing </description>
    </item>
    
  </channel>
</rss>