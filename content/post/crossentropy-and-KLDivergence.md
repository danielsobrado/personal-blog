---
description: "I our last post we've seen that entropy tells us the minimum number of bits that we need to encode our ground truth distribution.
What if we don't know the correct or ground truth distribution? In real life we have a approximated view of the reality, we cannot observe all events in most cases."
author: "Daniel Sobrado"
date: 2014-06-21
linktitle: Cross-Entropy and Kullback-Leibler Divergence
nomenu:
  main:
    parent: tutorials
prev: /tutorials/mathjax
next: /prior-posterior-probabilities
title: Cross-Entropy and Kullback-Leibler Divergence
noweight: 10
image: https://i.imgur.com/EG7uO7w.png
tags : [
    "cross-entropy",
    "kl-divergence"
]
categories : [
    "Maths"
]
---

# Cross-Entropy

I our last post we've seen that entropy tells us the minimum number of bits that we need to encode our ground truth distribution.

> We cannot encode our *correct* distribution with less information than the entropy.

What if we don't know the correct or ground truth distribution? In real life we have a approximated view of the reality, we cannot observe all events in most cases.

LetÂ´s call $ y $ to the ground truth and $ y_hat $ to our *incorrect* distribution.

# Kullback-Leibler Divergence




